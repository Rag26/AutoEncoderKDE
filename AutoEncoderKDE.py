# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z3Ebj_ryYl2A-IIMA7ug-LdlQmOPbS4H
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import tensorflow as tf

from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
import random
import os
import pandas as pd
from io import BytesIO

#Resize and Rescale the images
datagen = ImageDataGenerator(rescale=1./255)
SIZE = 128

#Mount Drive
from google.colab import drive
drive.mount('/content/drive')

#Create batches of the healthy train, healthy test, and unhealthy test images.

train_generator = datagen.flow_from_directory(
    '/content/drive/MyDrive/Colab Notebooks/archive (5).zip (Unzipped Files)/Training',
    target_size=(SIZE, SIZE),
    batch_size=64,
    class_mode='input'
    )
validation_generator = datagen.flow_from_directory(
    '/content/drive/MyDrive/Colab Notebooks/archive (5).zip (Unzipped Files)/Testing',
    target_size=(SIZE, SIZE),
    batch_size=64,
    class_mode='input'
  )
anomaly_generator = datagen.flow_from_directory(
    '/content/drive/MyDrive/Colab Notebooks/archive (5).zip (Unzipped Files)/Anomaly',
    target_size=(SIZE, SIZE),
    batch_size=64,
    class_mode='input'
    )

#Model Structure

model = Sequential()
model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(SIZE, SIZE, 3)))
model.add(MaxPooling2D((2, 2), padding='same'))
model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D((2, 2), padding='same'))
model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D((2, 2), padding='same'))
# model.add(Conv2D(8, (3, 3), activation='relu', padding='same'))
# model.add(MaxPooling2D((2, 2), padding='same'))
# model.add(Conv2D(4, (3, 3), activation='relu', padding='same'))
# model.add(MaxPooling2D((2, 2), padding='same'))
# model.add(Conv2D(8, (3, 3), activation='relu', padding='same'))
# model.add(MaxPooling2D((2, 2), padding='same'))

#Decoder
# model.add(Conv2D(8, (3, 3), activation='relu', padding='same'))
# model.add(UpSampling2D((2, 2)))
# model.add(Conv2D(4, (3, 3), activation='relu', padding='same'))
# model.add(UpSampling2D((2, 2)))
# model.add(Conv2D(8, (3, 3), activation='relu', padding='same'))
# model.add(UpSampling2D((2, 2)))
model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))
model.add(UpSampling2D((2, 2)))
model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model.add(UpSampling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
model.add(UpSampling2D((2, 2)))

model.add(Conv2D(3, (3, 3), activation='sigmoid', padding='same'))

model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])
model.summary()

#Train the model

history = model.fit(
        train_generator,
        steps_per_epoch= 395 // 64,
        epochs=5,
        validation_data=validation_generator,
        validation_steps=105 // 64,
        shuffle = True)

#Graph training and validation loss
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'y', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

#@title
data_batch = []  #Capture all training batches as a numpy array
img_num = 0
while img_num <= train_generator.batch_index:   #gets each generated batch of size batch_size
    data = train_generator.next()
    data_batch.append(data[0])
    img_num = img_num + 1

predicted = model.predict(data_batch[0])  #Predict on the first batch of images

#Print an input image and reconstruction
image_number = random.randint(0, predicted.shape[0])
plt.figure(figsize=(12, 6))
plt.subplot(121)
plt.imshow(data_batch[0][image_number])
plt.subplot(122)
plt.imshow(predicted[image_number])
plt.show()

#Evalute the model on all the images
validation_error = model.evaluate_generator(validation_generator)
anomaly_error = model.evaluate_generator(anomaly_generator)

print("Recon. error for the validation (normal) data is: ", validation_error)
print("Recon. error for the anomaly data is: ", anomaly_error)

#Create new model with just the encoder, same weights/biases
from tensorflow import keras
def passthrough(x):
    return x

encoder_model = Sequential()
encoder_model.add(keras.layers.Lambda(passthrough, input_shape=(None,)))

# encoder_model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(SIZE, SIZE, 3), weights=model.layers[0].get_weights()) )
# encoder_model.add(MaxPooling2D((2, 2), padding='same'))
# encoder_model.add(Conv2D(32, (3, 3), activation='relu', padding='same', weights=model.layers[2].get_weights()))
# encoder_model.add(MaxPooling2D((2, 2), padding='same'))
# encoder_model.add(Conv2D(16, (3, 3), activation='relu', padding='same', weights=model.layers[4].get_weights()))
# encoder_model.add(MaxPooling2D((2, 2), padding='same'))
# encoder_model.summary()

# Calculate KDE using sklearn
from sklearn.neighbors import KernelDensity

#Get encoded output of input images = Latent space
encoded_images = encoder_model.predict(train_generator)

# Flatten the encoder output because KDE from sklearn takes 1D vectors as input
# encoder_output_shape = encoder_model.output_shape #Here, we have 16x16x16
# out_vector_shape = encoder_output_shape[1]*encoder_output_shape[2]*encoder_output_shape[3]

out_vector_shape = 128*128*3
encoded_images_vector = [np.reshape(img, (out_vector_shape)) for img in encoded_images]



#Fit KDE to the image latent data
kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(encoded_images_vector)

#Encoder output
plt.figure(figsize=(12, 6))
plt.subplot(121)


plt.imshow(encoded_images[0][0], cmap = 'gray')

#Custom functin to calculate average loss, standard deviation of loss, average KDE score, and standard deviation of the KDE score
def calc_density_and_recon_error(batch_images):

    density_list=[]
    recon_error_list=[]
    for im in range(0, batch_images.shape[0]-1):

        img  = batch_images[im]
        img = img[np.newaxis, :,:,:]
        encoded_img = encoder_model.predict([[img]]) # Create a compressed version of the image using the encoder
        encoded_img = [np.reshape(img, (out_vector_shape)) for img in encoded_img] # Flatten the compressed image
        density = kde.score_samples(encoded_img)[0] # get a density score for the new image
        reconstruction = model.predict([[img]])
        reconstruction_error = model.evaluate([reconstruction],[[img]], batch_size = 1)[0]
        density_list.append(density)
        recon_error_list.append(reconstruction_error)

    average_density = np.mean(np.array(density_list))
    stdev_density = np.std(np.array(density_list))

    average_recon_error = np.mean(np.array(recon_error_list))
    stdev_recon_error = np.std(np.array(recon_error_list))

    return average_density, stdev_density, average_recon_error, stdev_recon_error

#Apply the above function on the images
train_batch = train_generator.next()[0]


uninfected_values = calc_density_and_recon_error(train_batch)

uninfected_values

#Custom function that utilizes the loss and KDE thresholds
def check_anomaly(img_path):
    anomaly_count = 0
    normal_count = 0
    density_threshold = 33900 #Set this value based of above
    reconstruction_error_threshold = 0.009 # Set this value based of above
    img  = Image.open(img_path)
    img = np.array(img.resize((128,128), Image.ANTIALIAS))
    plt.imshow(img)
    img = img / 255.
    img = img[np.newaxis, :,:,:]
    encoded_img = encoder_model.predict([[img]])
    encoded_img = [np.reshape(img, (out_vector_shape)) for img in encoded_img]
    density = kde.score_samples(encoded_img)[0]

    reconstruction = model.predict([[img]])
    reconstruction_error = model.evaluate([reconstruction],[[img]], batch_size = 1)[0]

    #or reconstruction_error > reconstruction_error_threshold density < density_threshold

    if  density < density_threshold:
        print("The image is an anomaly")
        anomaly_count+= 1

    else:
        print("The image is NOT an anomaly")
        normal_count+=1

    return normal_count, anomaly_count

#test Glioma Tumor Images
anomaly_sum = 0
normal_sum = 0
for i in range(94):
  i+=1
  normal_count, anomaly_count = check_anomaly('/content/drive/MyDrive/Colab Notebooks/archive (5).zip (Unzipped Files)/Anomaly/glioma_tumor/image('+str(i)+ ').jpg')
  anomaly_sum += anomaly_count
  normal_sum += normal_count

print(anomaly_sum, normal_sum)

#Test Meningioma Tumor images
anomaly_sum = 0
normal_sum = 0
for i in range(113):
  i+=1
  normal_count, anomaly_count = check_anomaly('/content/drive/MyDrive/Colab Notebooks/archive (5).zip (Unzipped Files)/Anomaly/meningioma_tumor/image('+str(i)+ ').jpg')
  anomaly_sum += anomaly_count
  normal_sum += normal_count
print(anomaly_sum, normal_sum)

#test piuitary tumor images
anomaly_sum = 0
normal_sum = 0
for i in range(73):
  i+=3
  normal_count, anomaly_count = check_anomaly('/content/drive/MyDrive/Colab Notebooks/archive (5).zip (Unzipped Files)/Anomaly/piuitary_tumor/image('+str(i)+ ').jpg')
  anomaly_sum += anomaly_count
  normal_sum += normal_count
print(anomaly_sum, normal_sum)

#test healthy images
anomaly_sum = 0
normal_sum = 0
for i in range(103):
  i+=1
  normal_count, anomaly_count = check_anomaly('/content/drive/MyDrive/Colab Notebooks/archive (5).zip (Unzipped Files)/Testing/no_tumor_test/image('+str(i)+ ').jpg')
  anomaly_sum += anomaly_count
  normal_sum += normal_count
print(anomaly_sum, normal_sum)

#test Alzheimerâ€™s images
import cv2
anomaly_sum = 0
normal_sum = 0
for i in range(50):
  i+=27
  img = cv2.imread('/content/drive/MyDrive/Colab Notebooks/archive (5).zip (Unzipped Files)/NoDe/32 ('+str(i)+ ').jpg', cv2.IMREAD_GRAYSCALE)

# Duplicate the channel
  img = np.dstack((img, img, img))

# Save the 3 channel image
  cv2.imwrite('3_channel_image.jpg', img)
  normal_count, anomaly_count = check_anomaly('3_channel_image.jpg')
  anomaly_sum += anomaly_count
  normal_sum += normal_count
print(anomaly_sum, normal_sum)